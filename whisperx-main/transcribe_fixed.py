import torch
import warnings
import sys
import os
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline

# ç¦ç”¨è­¦å‘Š
warnings.filterwarnings("ignore")

def clean_timestamps(chunks):
    """æ¸…ç†å’Œä¿®å¤æ—¶é—´æˆ³é‡å é—®é¢˜"""
    if not chunks:
        return []
    
    cleaned_chunks = []
    for i, chunk in enumerate(chunks):
        # è·å–æ—¶é—´æˆ³
        start_time = chunk['timestamp'][0] if chunk['timestamp'][0] is not None else 0
        end_time = chunk['timestamp'][1] if chunk['timestamp'][1] is not None else start_time + 1
        
        # ä¿®å¤é‡å é—®é¢˜ï¼šå¦‚æœå¼€å§‹æ—¶é—´å°äºå‰ä¸€ä¸ªç»“æŸæ—¶é—´ï¼Œè°ƒæ•´å¼€å§‹æ—¶é—´
        if cleaned_chunks and start_time < cleaned_chunks[-1]['end_time']:
            start_time = cleaned_chunks[-1]['end_time']
        
        # ç¡®ä¿ç»“æŸæ—¶é—´å¤§äºå¼€å§‹æ—¶é—´
        if end_time <= start_time:
            end_time = start_time + 1
        
        cleaned_chunks.append({
            'text': chunk['text'].strip(),
            'start_time': start_time,
            'end_time': end_time
        })
    
    return cleaned_chunks

def transcribe_audio_file(audio_path):
    """è½¬å½•çœŸå®éŸ³é¢‘æ–‡ä»¶ - ä¿®å¤ç‰ˆ"""
    
    if not os.path.exists(audio_path):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ {audio_path}")
        return None
    
    print("ğŸ¤ WhisperX å®é™…éŸ³é¢‘è½¬å½• (ä¿®å¤ç‰ˆ)")
    print("=" * 60)
    print(f"ğŸ“ éŸ³é¢‘æ–‡ä»¶: {audio_path}")
    
    # è®¾å¤‡é…ç½®
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
    model_id = "openai/whisper-large-v3-turbo"
    
    print(f"ğŸ“± è®¾å¤‡: {device}")
    print(f"ğŸ”§ æ¨¡å‹: {model_id}")
    
    # åŠ è½½æ¨¡å‹
    print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        model_id, 
        torch_dtype=torch_dtype, 
        low_cpu_mem_usage=True, 
        use_safetensors=True
    )
    model.to(device)
    
    processor = AutoProcessor.from_pretrained(model_id)
    
    # åˆ›å»ºç®¡é“
    pipe = pipeline(
        "automatic-speech-recognition",
        model=model,
        tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor,
        torch_dtype=torch_dtype,
        device=device,
    )
    
    print("ğŸµ å¼€å§‹è½¬å½•...")
    
    # è½¬å½•éŸ³é¢‘æ–‡ä»¶
    result = pipe(
        audio_path,
        generate_kwargs={"task": "transcribe"},
        return_timestamps=True,
        chunk_length_s=30,  # é™åˆ¶å—é•¿åº¦
        stride_length_s=5   # æ·»åŠ æ­¥é•¿ä»¥å‡å°‘é‡å 
    )
    
    print("\n" + "=" * 60)
    print("ğŸ“ è½¬å½•ç»“æœ:")
    print("=" * 60)
    print(result["text"])
    
    # æ¸…ç†å’Œæ˜¾ç¤ºæ—¶é—´æˆ³ä¿¡æ¯
    if "chunks" in result and result["chunks"]:
        cleaned_chunks = clean_timestamps(result["chunks"])
        
        print("\n" + "=" * 60)
        print("â° æ—¶é—´æˆ³åˆ†æ®µ (å·²ä¿®å¤):")
        print("=" * 60)
        
        for i, chunk in enumerate(cleaned_chunks, 1):
            print(f"[{i:2d}] {chunk['start_time']:6.2f}s - {chunk['end_time']:6.2f}s: {chunk['text']}")
        
        # æ˜¾ç¤ºæ€»æ—¶é•¿
        if cleaned_chunks:
            total_duration = max(chunk['end_time'] for chunk in cleaned_chunks)
            print(f"\nğŸ“Š æ€»æ—¶é•¿: {total_duration:.2f} ç§’")
    
    print("\nâœ… è½¬å½•å®Œæˆ!")
    return result["text"]

if __name__ == "__main__":
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python transcribe_fixed.py <éŸ³é¢‘æ–‡ä»¶è·¯å¾„>")
        print("æ”¯æŒçš„æ ¼å¼: .wav, .mp3, .flac, .m4a, .ogg ç­‰")
        print("ç¤ºä¾‹: python transcribe_fixed.py uploads/beach_description.wav")
        sys.exit(1)
    
    audio_file = sys.argv[1]
    transcribe_audio_file(audio_file)